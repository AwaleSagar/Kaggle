{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e21572b",
   "metadata": {},
   "source": [
    "## Titanic - Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94450c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imoprt data and viz libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load our Train and Test Data\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test  = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before starting with the EDA , lets do go for LayMan Approach. As our Kaggle Evaluation is based on Accuracy Score.\n",
    "\n",
    "## Layman Approach -- Assume all passengers in test set survived / dead \n",
    "\n",
    "## Assumption 0 all dead :(\n",
    "\n",
    "layman0_submission = test[[\"PassengerId\"]].copy()\n",
    "layman0_submission['Survived'] = 0\n",
    "layman0_submission.to_csv('../data/Laymam_all_dead.csv',index=False)\n",
    "\n",
    "# Assumption 1 all survived :D\n",
    "\n",
    "layman1_submission = test[[\"PassengerId\"]].copy()\n",
    "layman1_submission['Survived'] = 1\n",
    "layman1_submission.to_csv('../data/Laymam_all_survived.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590402c",
   "metadata": {},
   "source": [
    "+++ Results +++\n",
    "\n",
    "Public Score Accuracy\n",
    "\n",
    "- Layman approach all dead ----------- 0.62200\n",
    "- Layman approach all survived ------- 0.37799\n",
    "\n",
    "```\n",
    "\n",
    "Layman Conclusion :\n",
    "We can conclude that the total number of people who Died > total number of people survived for our test dataset. Just by using a Layman all-dead hypothesis test, we can achieve an accuracy of approximately 62% on our test data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe66ab",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis on training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## View sample of our training data\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb47348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get insights on training data\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89450f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get insights on test data\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d69faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check for Null values in training data \n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b48ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Check for Null values in testing data \n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5487ba",
   "metadata": {},
   "source": [
    "#### Plotting some Visualizations about the features on our training dataset -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to plot bar charts\n",
    "def bar_chart(feature):\n",
    "    survived = train[train['Survived']==1][feature].value_counts()\n",
    "    dead = train[train['Survived']==0][feature].value_counts()\n",
    "    df = pd.DataFrame([survived,dead])\n",
    "    df.index = ['Survived','Dead']\n",
    "    df.plot(kind='bar',stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77cf58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('Sex')\n",
    "bar_chart('Pclass')\n",
    "bar_chart('Embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9632d",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commbining train and test data for feature engg.\n",
    "all_data = [train,test]\n",
    "\n",
    "# Check Title of each individual on basis of their name\n",
    "for data in all_data :\n",
    "    data['Status'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "train.Status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ade26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Status of each individual on basis of their name\n",
    "for dataset in all_data:\n",
    "    ## Assign Rare title to all the vip personnel on the ship\n",
    "    dataset['Status'] = dataset['Status'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    ## Correct spelling mistakes in title for remaining individuals\n",
    "    dataset['Status'] = dataset['Status'].replace('Mlle', 'Miss')\n",
    "    dataset['Status'] = dataset['Status'].replace('Ms', 'Miss')\n",
    "    dataset['Status'] = dataset['Status'].replace('Mme', 'Mrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f064fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Status of each individual on basis of their name\n",
    "status_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "for dataset in all_data:\n",
    "    dataset['Status'] = dataset['Status'].map(status_mapping)\n",
    "    dataset['Status'] = dataset['Status'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b09ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping FamilySize column by summing Siblings + ParentChild + Individual \n",
    "train['FamilySize'] = train ['SibSp'] + train['Parch'] + 1\n",
    "test['FamilySize'] = test ['SibSp'] + test['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding isAlone column if individual was travelling without family    \n",
    "for dataset in all_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding Cabin Category\n",
    "train['Cabin_category'] = train['Cabin'].astype(str).str[0]\n",
    "train['Cabin_category'] = train['Cabin_category'].map({'A':1,'B':2,'C':2,'D':3,'E':4,'F':5,'G':6,'T':7})\n",
    "train['Cabin_category'] = train['Cabin_category'].fillna(0)\n",
    "# Cabin Grouping \n",
    "train['HasCabin'] = train['Cabin'].apply(lambda x:0 if x is np.nan else 1)\n",
    "\n",
    "\n",
    "test['Cabin_category'] = test['Cabin'].astype(str).str[0]\n",
    "test['Cabin_category'] = test['Cabin_category'].map({'A':1,'B':2,'C':2,'D':3,'E':4,'F':5,'G':6,'T':7})\n",
    "test['Cabin_category'] = test['Cabin_category'].fillna(0)\n",
    "# Cabin Grouping \n",
    "test['HasCabin'] = test['Cabin'].apply(lambda x:0 if x is np.nan else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbbb587",
   "metadata": {},
   "source": [
    "#### Filling in missing data on Age, Fare and Embarked features -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ec30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filling up missing Age values by taking a median\n",
    "train[\"Age\"].fillna(train.groupby(\"Status\")[\"Age\"].transform(\"median\"), inplace=True)\n",
    "test[\"Age\"].fillna(test.groupby(\"Status\")[\"Age\"].transform(\"median\"), inplace=True)\n",
    "\n",
    "\n",
    "## Filling up missing Fare values by taking a median on Pclass of an individual\n",
    "train['Fare'].fillna(train.groupby(['Pclass', 'Parch', 'SibSp'])['Fare'].median()[3][0][0], inplace = True)\n",
    "test['Fare'].fillna(test.groupby(['Pclass', 'Parch', 'SibSp'])['Fare'].median()[3][0][0], inplace = True)\n",
    "\n",
    "## Filling up missing Embarked values by replacing them with 'S' based on assumption that most people boarded from Southampton\n",
    "train['Embarked'].fillna('S', inplace = True)\n",
    "test['Embarked'].fillna('S', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4feb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for Null values in training data \n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a57e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for Null values in testing data \n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final insights on training Dataset\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629db89",
   "metadata": {},
   "source": [
    "#### Training our model and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19374f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models from SCikit -Learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report , accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59820073",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = train[\"Survived\"]\n",
    "\n",
    "features = [\"Pclass\",\"Sex\", \"Age\",\"Fare\",\"Embarked\",\"IsAlone\", \"FamilySize\", \"Status\",\"Cabin_category\",\"HasCabin\"]\n",
    "\n",
    "X_full = pd.get_dummies(train[features])\n",
    "X_test_full = pd.get_dummies(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting our dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, train_size=0.7, test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for different models\n",
    "\n",
    "models = { \"Logistic Regression\": LogisticRegression(),\n",
    "            \"Naive Bayes\": GaussianNB(),\n",
    "            \"Stochastic Gradient\": SGDClassifier(),\n",
    "            \"KNeighbors Classifier\": KNeighborsClassifier(),\n",
    "            \"DecisionTree Classifier\": DecisionTreeClassifier(),\n",
    "            \"RandomForest Classifier\": RandomForestClassifier(),\n",
    "            \"Support Vector Machine\": SVC()}\n",
    "\n",
    "\n",
    "\n",
    "# Create a function to fit and score models\n",
    "def fit_and_score(models, X_train,X_test,y_train,y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models.\n",
    "    models: a dict of different SCikit-Learn machine learning models\n",
    "    X_train : training data(no labels)\n",
    "    X_test: testing data (no labels)\n",
    "    y_train: training labels\n",
    "    y_test: test labels\n",
    "    \"\"\"\n",
    "    # set random seeed\n",
    "    np.random.seed(42)\n",
    "    # Make a dictionary to keep model scores\n",
    "    model_scores = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train,y_train)\n",
    "        # Evaluate the model and append its score to model scores\n",
    "        model_scores[name] = model.score(X_test,y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cf243",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train our data on different classifiers \n",
    "\n",
    "model_scores = fit_and_score(models=models,\n",
    "                             X_train= X_train,\n",
    "                             X_test= X_test,\n",
    "                            y_train=y_train,\n",
    "                            y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\n",
    "model_compare.T.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa803ae5",
   "metadata": {},
   "source": [
    "####  Hyerparameter Tuning for Random Forest model as we have best acc score for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyperparameter grid for RandomForestClassifier\n",
    "rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n",
    "           \"max_depth\": np.arange(1, 50, 2),\n",
    "           \"min_samples_split\": np.arange(2, 50, 2),\n",
    "           \"min_samples_leaf\": np.arange(1, 20, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune RandomForestClassifier\n",
    "# Setup random seed\n",
    "np.random.seed(3)\n",
    "\n",
    "# Setup random hyperparameter search for RandomForestClassifier\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                           param_distributions=rf_grid,\n",
    "                           cv=5,\n",
    "                           n_iter=20,\n",
    "                           verbose=True, n_jobs=-1)\n",
    "\n",
    "# Fit random hyperparameter search model for RandomForestClassifier()\n",
    "rs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ebd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best hyperparameters\n",
    "rs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab5010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the randomized search RandomForestClassifier model\n",
    "rs_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae20d0",
   "metadata": {},
   "source": [
    "#### Hyperparamter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different hyperparameters for our LogisticRegression model\n",
    "rf_grid = {\"n_estimators\": np.arange(100, 400, 50),\n",
    "           \"max_depth\": np.arange(10,20, 2),\n",
    "           \"min_samples_split\": np.arange(20,30, 2),\n",
    "           \"min_samples_leaf\": np.arange(2, 10, 2),\n",
    "          \"random_state\": [0] }\n",
    "\n",
    "# Setup grid hyperparameter search for LogisticRegression\n",
    "gs_rf = GridSearchCV(RandomForestClassifier(),\n",
    "                          param_grid=rf_grid,\n",
    "                          cv=5,\n",
    "                          verbose=True,n_jobs=-1)\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e902a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best hyperparmaters\n",
    "gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4a131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the GridSearchCV search RandomForestClassifier model\n",
    "gs_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with tuned model\n",
    "y_preds = gs_rf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143aa110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Increase font size\n",
    "sns.set(font_scale=1.5) \n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True label\") # true labels go on the y-axis \n",
    "    \n",
    "plot_conf_mat(y_valid, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4719fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06340c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier with best parameters\n",
    "clf = RandomForestClassifier(max_depth= 18,\n",
    " min_samples_leaf= 2,\n",
    " min_samples_split = 26,\n",
    " n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d63473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated accuracy\n",
    "cv_acc = cross_val_score(clf,\n",
    "                         X_full,\n",
    "                         y_full,\n",
    "                         cv=5,\n",
    "                         scoring=\"accuracy\")\n",
    "cv_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a56f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated precision\n",
    "cv_precision = cross_val_score(clf,\n",
    "                         X_full,\n",
    "                         y_full,\n",
    "                         cv=5,\n",
    "                         scoring=\"precision\")\n",
    "cv_precision=np.mean(cv_precision)\n",
    "cv_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated recall\n",
    "cv_recall = cross_val_score(clf,\n",
    "                         X_full,\n",
    "                         y_full,\n",
    "                         cv=5,\n",
    "                         scoring=\"recall\")\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f91b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated f1-score\n",
    "cv_f1 = cross_val_score(clf,\n",
    "                         X_full,\n",
    "                         y_full,\n",
    "                         cv=5,\n",
    "                         scoring=\"f1\")\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n",
    "                           \"Precision\": cv_precision,\n",
    "                           \"Recall\": cv_recall,\n",
    "                           \"F1\": cv_f1},\n",
    "                          index=[0])\n",
    "\n",
    "cv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n",
    "                      legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb47b7",
   "metadata": {},
   "source": [
    "#### Final RF model with Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586714a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators = 500, max_depth = 10, min_samples_split = 6 )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "rf_val_predictions = clf.predict(X_valid)\n",
    "\n",
    "rf_accuracy = accuracy_score(rf_val_predictions,y_valid)\n",
    "\n",
    "rf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c9faa",
   "metadata": {},
   "source": [
    "#### Predicting and Exporting our final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc78016",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_full, y_full)\n",
    "predictions = clf.predict(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('../data/Tuned_RF_Model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089bbc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35bc01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
